#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

version: "3.8"

services:
  ## Redis
  apisix_redis:
    # The latest image is the latest stable version
    image: redis:latest
    restart: unless-stopped
    ports:
      - "6379:6379"
    networks:
      apisix_net:

  ## kafka-cluster
  zookeeper-server1:
    image: zookeeper:3.6
    restart: unless-stopped
    ports:
      - "2181:2181"
    networks:
      kafka_net:

  zookeeper-server2:
    image: zookeeper:3.6
    restart: unless-stopped
    ports:
      - "12181:12181"
    networks:
      kafka_net:

  kafka-server1:
    image: wurstmeister/kafka:2.13-2.8.1
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-server1:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_CREATE_TOPICS: "test2:1:1,test3:3:1"
      KAFKA_LISTENERS: PLAINTEXT://:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://127.0.0.1:9092
    restart: unless-stopped
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper-server1
      - zookeeper-server2
    networks:
      kafka_net:

  kafka-server2:
    image: wurstmeister/kafka:2.13-2.8.1
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-server2:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_CREATE_TOPICS: "test4:1:1"
      KAFKA_LISTENERS: PLAINTEXT://:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://127.0.0.1:9092
    restart: unless-stopped
    ports:
      - "19092:9092"
    depends_on:
      - zookeeper-server1
      - zookeeper-server2
    networks:
      kafka_net:


  ## keycloak
  apisix_keycloak:
    image: mihaibob/keycloak:17.0.0-legacy
    environment:
      KEYCLOAK_USER: admin
      KEYCLOAK_PASSWORD: 123456
      KEYCLOAK_IMPORT: "/tmp/University-realm.json -Dkeycloak.profile.feature.upload_scripts=enabled"
    restart: unless-stopped
    volumes:
      - ./ci/pod/keycloak/University-realm.json:/tmp/University-realm.json
    ports:
      - "8090:8080"
      - "8443:8443"
    networks:
      apisix_net:

  ## SkyWalking
  skywalking:
    image: apache/skywalking-oap-server:8.9.0
    restart: unless-stopped
    ports:
      - "1234:1234"
      - "11800:11800"
      - "12800:12800"
    networks:
      skywalk_net:


  ## Consul
  consul_1:
    image: consul:1.7
    restart: unless-stopped
    ports:
      - "8500:8500"
    command: [ "consul", "agent", "-server", "-bootstrap-expect=1", "-client", "0.0.0.0", "-log-level", "info", "-data-dir=/consul/data" ]
    networks:
      consul_net:

  consul_2:
    image: consul:1.7
    restart: unless-stopped
    ports:
      - "8600:8500"
    command: [ "consul", "agent", "-server", "-bootstrap-expect=1", "-client", "0.0.0.0", "-log-level", "info", "-data-dir=/consul/data" ]
    networks:
      consul_net:


  ## HashiCorp Vault
  vault:
    image: vault:1.9.0
    container_name: vault
    restart: unless-stopped
    ports:
      - "8200:8200"
    cap_add:
      - IPC_LOCK
    environment:
      VAULT_DEV_ROOT_TOKEN_ID: root
      VAULT_DEV_LISTEN_ADDRESS: 0.0.0.0:8200
    command: [ "vault", "server", "-dev" ]
    networks:
      vault_net:


  ## Nacos cluster
  nacos_auth:
    hostname: nacos1
    image: nacos/nacos-server:2.0.4-slim
    env_file:
      - ci/pod/nacos/env/common.env
    environment:
      NACOS_AUTH_ENABLE: "true"
    restart: unless-stopped
    ports:
      - "8848:8848"
    networks:
      nacos_net:

  nacos_no_auth:
    hostname: nacos2
    image: nacos/nacos-server:2.0.4-slim
    env_file:
      - ci/pod/nacos/env/common.env
    restart: unless-stopped
    ports:
      - "8858:8848"
    networks:
      nacos_net:

  nacos_server_health_check:
    build:
      context: ci/pod/nacos/healthcheck
      dockerfile: Dockerfile
    environment:
      CHECK_URI: "http://nacos2:8848/nacos/v1/ns/service/list?pageNo=1&pageSize=2"
    tty: true
    # debug healthcheck script
#    volumes:
#     - ./ci/pod/nacos/healthcheck/nacos-server-healthcheck.sh:/nacos-server-healthcheck.sh
    healthcheck:
      test: [ "CMD", "bash", "/nacos-server-healthcheck.sh" ]
      interval: 5s
      timeout: 5s
      retries: 60
      start_period: 10s
    networks:
      nacos_net:

  nacos_service_health_check:
    build:
      context: ci/pod/nacos/healthcheck
      dockerfile: Dockerfile
    # debug healthcheck script
#    volumes:
#     - ./ci/pod/nacos/healthcheck/nacos-service-healthcheck.sh:/nacos-service-healthcheck.sh
    tty: true
    healthcheck:
      test: [ "CMD", "bash", "/nacos-service-healthcheck.sh" ]
      interval: 5s
      timeout: 30s
      retries: 60
      start_period: 10s
    networks:
      nacos_net:

  ### Nacos services
  nacos-service1:
    build:
      context: ci/pod/nacos/service
      dockerfile: Dockerfile
    env_file:
      - ci/pod/nacos/env/service.env
    environment:
      SUFFIX_NUM: 1
    restart: unless-stopped
    ports:
      - "18001:18001"
    depends_on:
      nacos_server_health_check:
        condition: service_healthy
    networks:
      nacos_net:

  nacos-service2:
    build:
      context: ci/pod/nacos/service
      dockerfile: Dockerfile
    env_file:
      - ci/pod/nacos/env/service.env
    environment:
      SUFFIX_NUM: 2
    restart: unless-stopped
    ports:
      - "18002:18001"
    depends_on:
      nacos_server_health_check:
        condition: service_healthy
    networks:
      nacos_net:

  nacos-service3:
    build:
      context: ci/pod/nacos/service
      dockerfile: Dockerfile
    env_file:
      - ci/pod/nacos/env/service.env
    environment:
      SUFFIX_NUM: 1
      NAMESPACE: test_ns
    restart: unless-stopped
    ports:
      - "18003:18001"
    depends_on:
      nacos_server_health_check:
        condition: service_healthy
    networks:
      nacos_net:

  nacos-service4:
    build:
      context: ci/pod/nacos/service
      dockerfile: Dockerfile
    env_file:
      - ci/pod/nacos/env/service.env
    environment:
      SUFFIX_NUM: 1
      GROUP: test_group
    restart: unless-stopped
    ports:
      - "18004:18001"
    depends_on:
      nacos_server_health_check:
        condition: service_healthy
    networks:
      nacos_net:

  nacos-service5:
    build:
      context: ci/pod/nacos/service
      dockerfile: Dockerfile
    env_file:
      - ci/pod/nacos/env/service.env
    environment:
      SUFFIX_NUM: 1
      GROUP: test_group
      NAMESPACE: test_ns
    restart: unless-stopped
    ports:
      - "18005:18001"
    depends_on:
      nacos_server_health_check:
        condition: service_healthy
    networks:
      nacos_net:

  nacos-service6:
    build:
      context: ci/pod/nacos/service
      dockerfile: Dockerfile
    env_file:
      - ci/pod/nacos/env/service.env
    environment:
      SUFFIX_NUM: 3
      GROUP: test_group2
      NAMESPACE: test_ns
    restart: unless-stopped
    ports:
      - "18006:18001"
    depends_on:
      nacos_server_health_check:
        condition: service_healthy
    networks:
      nacos_net:

  nacos-service7:
    build:
      context: ci/pod/nacos/service
      dockerfile: Dockerfile
    env_file:
      - ci/pod/nacos/env/service.env
    environment:
      SUFFIX_NUM: 4
      GROUP: test_group
      NAMESPACE: test_ns2
    restart: unless-stopped
    ports:
      - "18007:18001"
    depends_on:
      nacos_server_health_check:
        condition: service_healthy
    networks:
      nacos_net:


networks:
  apisix_net:
  kafka_net:
  nacos_net:
  skywalk_net:
  consul_net:
  vault_net:
